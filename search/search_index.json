{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"EveNet Documentation Portal \u00b6 Welcome to the EveNet knowledge base! This site hosts the same Markdown guides that live in the repository, but it formats them with navigation, search, and table-of-contents support so new collaborators can onboard quickly. Use the navigation menu to jump straight to the guide you need, or start with the highlights below. \ud83d\ude80 Quick Starts \u00b6 Plug-and-play setup? Follow the Quick Start path in the Getting Started tutorial to pair the official Docker image with the PyPI package. Hacking on the source? The same guide outlines the advanced workflow for cloning the repo and running modules directly. Prepping datasets? Head to the Data Preparation guide to learn how to configure preprocessing YAMLs and generate parquet shards. Training & inference. Consult the Training playbook and Prediction walkthrough for command-line examples and Ray configuration tips. \ud83e\udde0 Reference Library \u00b6 Architecture deep dive. The Model Architecture overview explains the hybrid point-cloud and global feature encoders that power EveNet. Configuration catalogue. The Configuration reference documents every YAML option and how they interact. Internal utilities. Project maintainers can review internal preprocessing notes for NERSC-specific helpers. \ud83d\udcda Resources \u00b6 Here are the main resources for EveNet, collected in one place for quick access: \ud83d\udc33 Docker image : docker.io/avencast1994/evenet:1.5 \ud83d\udc0d Python package : PyPI evenet \ud83c\udfcb\ufe0f Pretrained weights : Hugging Face Model Hub \ud83d\udce6 Pretraining dataset : Hugging Face Datasets \ud83d\udcbb Git repository : UW-EPE-ML/EveNet_Public \ud83d\udcc4 arXiv paper : coming soon","title":"Home"},{"location":"#evenet-documentation-portal","text":"Welcome to the EveNet knowledge base! This site hosts the same Markdown guides that live in the repository, but it formats them with navigation, search, and table-of-contents support so new collaborators can onboard quickly. Use the navigation menu to jump straight to the guide you need, or start with the highlights below.","title":"EveNet Documentation Portal"},{"location":"#quick-starts","text":"Plug-and-play setup? Follow the Quick Start path in the Getting Started tutorial to pair the official Docker image with the PyPI package. Hacking on the source? The same guide outlines the advanced workflow for cloning the repo and running modules directly. Prepping datasets? Head to the Data Preparation guide to learn how to configure preprocessing YAMLs and generate parquet shards. Training & inference. Consult the Training playbook and Prediction walkthrough for command-line examples and Ray configuration tips.","title":"\ud83d\ude80 Quick Starts"},{"location":"#reference-library","text":"Architecture deep dive. The Model Architecture overview explains the hybrid point-cloud and global feature encoders that power EveNet. Configuration catalogue. The Configuration reference documents every YAML option and how they interact. Internal utilities. Project maintainers can review internal preprocessing notes for NERSC-specific helpers.","title":"\ud83e\udde0 Reference Library"},{"location":"#resources","text":"Here are the main resources for EveNet, collected in one place for quick access: \ud83d\udc33 Docker image : docker.io/avencast1994/evenet:1.5 \ud83d\udc0d Python package : PyPI evenet \ud83c\udfcb\ufe0f Pretrained weights : Hugging Face Model Hub \ud83d\udce6 Pretraining dataset : Hugging Face Datasets \ud83d\udcbb Git repository : UW-EPE-ML/EveNet_Public \ud83d\udcc4 arXiv paper : coming soon","title":"\ud83d\udcda Resources"},{"location":"configuration/","text":"\u2699\ufe0f Configuration Reference \u00b6 Welcome to the EveNet control center! This document explains how the YAML examples under share/*-example.yaml fit together, how defaults are merged, and which knobs to tweak for your experiments. Loader basics Tour of share/finetune-example.yaml Options deep dive Network templates Physics metadata Prediction-specific notes Creating a new experiment \ud83e\uddf0 Loader Basics \u00b6 Top-level scripts expect a single YAML path as their positional argument: python evenet/train.py share/finetune-example.yaml python evenet/predict.py share/predict-example.yaml Inside the script, evenet/control/global_config.py handles parsing: Config.load_yaml(path) reads the YAML file. For any section containing a default: key, the loader first reads that template (relative to the YAML file) and then merges inline overrides. event_info and resonance are combined into an EventInfo instance so downstream code can access schemas, resonance trees, and process metadata by attribute ( config.event_info.regression_names , etc.). Other sections remain accessible as nested DotDict objects, which means attribute access (e.g., config.options.Training.epochs ) works as expected. \ud83d\udd01 Want to inspect the merged configuration? Call global_config.display() inside a script to print a rich table of overrides versus defaults. \ud83d\udcc4 Tour of share/finetune-example.yaml \u00b6 This example highlights the main sections you will encounter: Section What it controls platform Ray cluster parameters: worker count, per-worker CPU/GPU resources, batch size, and the data_parquet_dir used to discover shards. logger Logging destinations, including Weights & Biases and the optional local logger. Anchors let you reuse names across loggers. options Points to options/options.yaml for defaults, then overrides epochs, checkpoints, and component toggles. Dataset Dataset-wide overrides like normalization_file , dataset subsampling, and validation split. network Which architecture template to load ( network/network-20M.yaml , for example) plus any inline tweaks. event_info , resonance Canonical particle definitions, resonance trees, and symmetries used during preprocessing and by the model. For inference, share/predict-example.yaml mirrors the structure but requires Training.model_checkpoint_load_path and defines an options.prediction block for output writers. Platform Keys \u00b6 Key Description data_parquet_dir Folder containing data_*.parquet and shape_metadata.json . number_of_workers Ray workers to launch. Adjust alongside resources_per_worker . resources_per_worker CPU/GPU allocation per worker (e.g., {CPU: 1, GPU: 1} ). batch_size , prefetch_batches Passed to Ray Data\u2019s iter_torch_batches loader. use_gpu Toggle GPU usage for CPU-only runs. Dataset Block \u00b6 Key Meaning dataset_limit Fraction of the shuffled dataset to use (1.0 = full dataset). normalization_file Absolute path to normalization.pt from preprocessing. val_split [start, end] fraction describing the validation window. \ud83e\udde9 Options Deep Dive \u00b6 share/options/options.yaml collects optimizer groups, scheduler defaults, and component toggles. Use the top-level options block in your example file to override any field. Training Settings \u00b6 Field Description total_epochs / epochs Planned total vs. epochs executed in the current run. learning_rate , weight_decay Shared defaults broadcast to component blocks. model_checkpoint_save_path Directory for Lightning checkpoints. model_checkpoint_load_path Resume training from this checkpoint (set to null to start fresh). pretrain_model_load_path Load weights for fine-tuning without resuming optimizer state. diffusion_every_n_epochs , diffusion_every_n_steps Validation cadence for diffusion-based heads. eval_metrics_every_n_epochs Controls how often expensive metrics are computed. apply_event_weight When true , multiplies losses by per-event weights from preprocessing. Component Blocks \u00b6 Each component inherits optimizer settings and can be toggled independently: Component Notable knobs GlobalEmbedding , PET , ObjectEncoder Optimizer group, warm-up behavior, layer freezing. Classification , Regression , Assignment , Segmentation include flags, loss scales, attention/decoder depth, mask usage. GlobalGeneration , ReconGeneration , TruthGeneration Diffusion steps, reuse settings, optimizer routing. Progressive Training \u00b6 The ProgressiveTraining section defines staged curricula. Each stage supplies an epoch_ratio , optional transition_ratio , and start/end values for parameters (loss weights, noise probabilities, EMA decay). Modify stages to ramp tasks gradually or disable them for single-stage training. EMA & Early Stopping \u00b6 Within options.Training you will find: EMA \u2013 enable/disable exponential moving averages, set decay, start epoch, and whether to swap EMA weights into the model after loading or at run completion. EarlyStopping \u2013 Lightning-compatible patience, monitored metric, comparison mode, and verbosity. \ud83c\udfd7\ufe0f Network Templates \u00b6 Files under share/network/ specify architectural hyperparameters for the shared body and task heads. For example, share/network/network-20M.yaml defines transformer depth, attention heads, neighborhood sizes, and diffusion dimensions. Override specific fields inside the network section of your top-level YAML to experiment without editing the base template. \ud83e\udde0 Combine template overrides with component toggles: disable Body.PET.feature_drop during fine-tuning, or shrink head hidden sizes for small datasets. \ud83e\uddec Physics Metadata \u00b6 Event Info \u00b6 share/event_info/multi_process.yaml describes: Inputs \u2013 sequential particle features and global condition vectors, including normalization strategies. Resonance topology \u2013 parent/daughter relationships for each process, powering assignments and segmentation targets. Regression & segmentation catalogs \u2013 enumerations of momenta and particle groups used by the respective heads. Process Info \u00b6 share/process_info/default.yaml captures per-process metadata: category names for grouping. process_id integers stored with each event. diagram entries referencing resonance definitions and symmetry hints. Optional rename_to fields for remapping process names during preprocessing. Ensure the same event/process files are used in preprocessing and training so label ordering stays consistent. Resonance Catalogs \u00b6 Templates under share/resonance/ hold reusable resonance definitions. Reference them from event_info to avoid duplicating decay trees across configs. \ud83d\udce6 Prediction Extras \u00b6 share/predict-example.yaml includes an options.prediction block where you can specify the output directory, filename, and any extra tensors to persist. The platform section mirrors training but is often tuned for throughput (e.g., more workers with smaller batch sizes). Always set Training.model_checkpoint_load_path to the checkpoint you want to score. \ud83c\udd95 Creating a New Experiment \u00b6 Copy an example \u2013 duplicate share/finetune-example.yaml (or share/predict-example.yaml ). Update paths \u2013 fill in platform.data_parquet_dir , options.Dataset.normalization_file , and logging/checkpoint directories. Toggle components \u2013 adjust options.Training.Components to match the supervision you have. Tweak network settings \u2013 override fields in the network section as needed. Track metadata \u2013 if you add new processes or features, update the relevant YAMLs under share/event_info/ and share/process_info/ , then rerun preprocessing so tensors stay aligned. Happy experimenting! \ud83e\uddea","title":"Configuration Reference"},{"location":"configuration/#configuration-reference","text":"Welcome to the EveNet control center! This document explains how the YAML examples under share/*-example.yaml fit together, how defaults are merged, and which knobs to tweak for your experiments. Loader basics Tour of share/finetune-example.yaml Options deep dive Network templates Physics metadata Prediction-specific notes Creating a new experiment","title":"\u2699\ufe0f Configuration Reference"},{"location":"configuration/#loader-basics","text":"Top-level scripts expect a single YAML path as their positional argument: python evenet/train.py share/finetune-example.yaml python evenet/predict.py share/predict-example.yaml Inside the script, evenet/control/global_config.py handles parsing: Config.load_yaml(path) reads the YAML file. For any section containing a default: key, the loader first reads that template (relative to the YAML file) and then merges inline overrides. event_info and resonance are combined into an EventInfo instance so downstream code can access schemas, resonance trees, and process metadata by attribute ( config.event_info.regression_names , etc.). Other sections remain accessible as nested DotDict objects, which means attribute access (e.g., config.options.Training.epochs ) works as expected. \ud83d\udd01 Want to inspect the merged configuration? Call global_config.display() inside a script to print a rich table of overrides versus defaults.","title":"\ud83e\uddf0 Loader Basics"},{"location":"configuration/#tour-of-sharefinetune-exampleyaml","text":"This example highlights the main sections you will encounter: Section What it controls platform Ray cluster parameters: worker count, per-worker CPU/GPU resources, batch size, and the data_parquet_dir used to discover shards. logger Logging destinations, including Weights & Biases and the optional local logger. Anchors let you reuse names across loggers. options Points to options/options.yaml for defaults, then overrides epochs, checkpoints, and component toggles. Dataset Dataset-wide overrides like normalization_file , dataset subsampling, and validation split. network Which architecture template to load ( network/network-20M.yaml , for example) plus any inline tweaks. event_info , resonance Canonical particle definitions, resonance trees, and symmetries used during preprocessing and by the model. For inference, share/predict-example.yaml mirrors the structure but requires Training.model_checkpoint_load_path and defines an options.prediction block for output writers.","title":"\ud83d\udcc4 Tour of share/finetune-example.yaml"},{"location":"configuration/#platform-keys","text":"Key Description data_parquet_dir Folder containing data_*.parquet and shape_metadata.json . number_of_workers Ray workers to launch. Adjust alongside resources_per_worker . resources_per_worker CPU/GPU allocation per worker (e.g., {CPU: 1, GPU: 1} ). batch_size , prefetch_batches Passed to Ray Data\u2019s iter_torch_batches loader. use_gpu Toggle GPU usage for CPU-only runs.","title":"Platform Keys"},{"location":"configuration/#dataset-block","text":"Key Meaning dataset_limit Fraction of the shuffled dataset to use (1.0 = full dataset). normalization_file Absolute path to normalization.pt from preprocessing. val_split [start, end] fraction describing the validation window.","title":"Dataset Block"},{"location":"configuration/#options-deep-dive","text":"share/options/options.yaml collects optimizer groups, scheduler defaults, and component toggles. Use the top-level options block in your example file to override any field.","title":"\ud83e\udde9 Options Deep Dive"},{"location":"configuration/#training-settings","text":"Field Description total_epochs / epochs Planned total vs. epochs executed in the current run. learning_rate , weight_decay Shared defaults broadcast to component blocks. model_checkpoint_save_path Directory for Lightning checkpoints. model_checkpoint_load_path Resume training from this checkpoint (set to null to start fresh). pretrain_model_load_path Load weights for fine-tuning without resuming optimizer state. diffusion_every_n_epochs , diffusion_every_n_steps Validation cadence for diffusion-based heads. eval_metrics_every_n_epochs Controls how often expensive metrics are computed. apply_event_weight When true , multiplies losses by per-event weights from preprocessing.","title":"Training Settings"},{"location":"configuration/#component-blocks","text":"Each component inherits optimizer settings and can be toggled independently: Component Notable knobs GlobalEmbedding , PET , ObjectEncoder Optimizer group, warm-up behavior, layer freezing. Classification , Regression , Assignment , Segmentation include flags, loss scales, attention/decoder depth, mask usage. GlobalGeneration , ReconGeneration , TruthGeneration Diffusion steps, reuse settings, optimizer routing.","title":"Component Blocks"},{"location":"configuration/#progressive-training","text":"The ProgressiveTraining section defines staged curricula. Each stage supplies an epoch_ratio , optional transition_ratio , and start/end values for parameters (loss weights, noise probabilities, EMA decay). Modify stages to ramp tasks gradually or disable them for single-stage training.","title":"Progressive Training"},{"location":"configuration/#ema-early-stopping","text":"Within options.Training you will find: EMA \u2013 enable/disable exponential moving averages, set decay, start epoch, and whether to swap EMA weights into the model after loading or at run completion. EarlyStopping \u2013 Lightning-compatible patience, monitored metric, comparison mode, and verbosity.","title":"EMA &amp; Early Stopping"},{"location":"configuration/#network-templates","text":"Files under share/network/ specify architectural hyperparameters for the shared body and task heads. For example, share/network/network-20M.yaml defines transformer depth, attention heads, neighborhood sizes, and diffusion dimensions. Override specific fields inside the network section of your top-level YAML to experiment without editing the base template. \ud83e\udde0 Combine template overrides with component toggles: disable Body.PET.feature_drop during fine-tuning, or shrink head hidden sizes for small datasets.","title":"\ud83c\udfd7\ufe0f Network Templates"},{"location":"configuration/#physics-metadata","text":"","title":"\ud83e\uddec Physics Metadata"},{"location":"configuration/#event-info","text":"share/event_info/multi_process.yaml describes: Inputs \u2013 sequential particle features and global condition vectors, including normalization strategies. Resonance topology \u2013 parent/daughter relationships for each process, powering assignments and segmentation targets. Regression & segmentation catalogs \u2013 enumerations of momenta and particle groups used by the respective heads.","title":"Event Info"},{"location":"configuration/#process-info","text":"share/process_info/default.yaml captures per-process metadata: category names for grouping. process_id integers stored with each event. diagram entries referencing resonance definitions and symmetry hints. Optional rename_to fields for remapping process names during preprocessing. Ensure the same event/process files are used in preprocessing and training so label ordering stays consistent.","title":"Process Info"},{"location":"configuration/#resonance-catalogs","text":"Templates under share/resonance/ hold reusable resonance definitions. Reference them from event_info to avoid duplicating decay trees across configs.","title":"Resonance Catalogs"},{"location":"configuration/#prediction-extras","text":"share/predict-example.yaml includes an options.prediction block where you can specify the output directory, filename, and any extra tensors to persist. The platform section mirrors training but is often tuned for throughput (e.g., more workers with smaller batch sizes). Always set Training.model_checkpoint_load_path to the checkpoint you want to score.","title":"\ud83d\udce6 Prediction Extras"},{"location":"configuration/#creating-a-new-experiment","text":"Copy an example \u2013 duplicate share/finetune-example.yaml (or share/predict-example.yaml ). Update paths \u2013 fill in platform.data_parquet_dir , options.Dataset.normalization_file , and logging/checkpoint directories. Toggle components \u2013 adjust options.Training.Components to match the supervision you have. Tweak network settings \u2013 override fields in the network section as needed. Track metadata \u2013 if you add new processes or features, update the relevant YAMLs under share/event_info/ and share/process_info/ , then rerun preprocessing so tensors stay aligned. Happy experimenting! \ud83e\uddea","title":"\ud83c\udd95 Creating a New Experiment"},{"location":"data_preparation/","text":"\ud83e\uddea Data Preparation & Updated Input Reference \u00b6 This page documents the new preprocessing contract for EveNet. The reference schema in share/event_info/pretrain.yaml is bundled with the repository as an example, but you are free to adapt the feature lists to match your campaign. The goal of this guide is to help you build the .npz files that the EveNet converter ingests and to clarify how each tensor maps onto the model heads. \u26a0\ufe0f Ownership reminder: Users are responsible for generating the .npz files. EveNet does not reorder or reshape features for you\u2014the arrays must already follow the size and ordering implied by your event-info YAML. The converter simply validates the layout and writes the parquet outputs. Keep the YAML and the .npz dictionary in sync at all times. Config + CLI workflow Input tensor dictionary Supervision targets by head NPZ \u2192 Parquet conversion Runtime checklist \ud83d\udee0\ufe0f Config + CLI Workflow \u00b6 Start from an event-info YAML. The repository ships an example at share/event_info/pretrain.yaml ; copy or extend it to describe the objects, global variables, and heads you plan to enable. The display names inside the INPUTS block are just labels used for logging and plotting\u2014what matters is the order, which must match the tensor layout you write to disk. Produce an event dictionary. For every event, assemble a Python dictionary that satisfies the shapes described below and append it to the archive you will write to disk. When you call numpy.savez (or savez_compressed ), each key becomes an array with leading dimension N , the number of events in the file. Masks indicate which padded entries are valid and should contribute to the loss. Run the EveNet converter. Point preprocessing/preprocess.py at your .npz bundle and pass the matching YAML so the loader can recover feature names, the number of sequential vectors, and the heads you are enabling. The converter assumes both artifacts describe the same structure\u2014mismatches will surface as validation errors. Two ingest modes are supported: Explicit file splits : provide train/val/test files directly so the converter preserves your boundaries. On-the-fly splits : provide one or more unsplit files and let the converter perform an event-level split using your --split_ratio . Train or evaluate. Training configs reference the resulting parquet directory via platform.data_parquet_dir and reuse the same YAML in options.Dataset.event_info . \u2728 Normalization note. Features marked log_normalize in the YAML are converted with np.log1p during preprocessing (converter logs the affected columns). Values must be finite and non-negative or the run will fail so you can fix the upstream pipeline. Other tags ( normalize , none ) remain metadata only, while normalize_uniform still denotes the wrapped representation for circular variables ( \u03c6 ). \ud83d\udce6 Input Tensor Dictionary \u00b6 Each event is described by the following feature tensors. Shapes are shown with a leading N to indicate the number of stored events in a given .npz file. Masks share the same leading dimension as the value they gate. During conversion, preprocessing/sanity_checks.py logs these expectations and will cast dtypes when possible so you can quickly verify the layout coming from your pipeline. Key Shape Description num_vectors (N,) Total count of global + sequential objects per event. num_sequential_vectors (N,) Number of valid sequential entries per event. Mirrors num_vectors behaviour. x (N, 18, 7) Point-cloud tensor storing exactly 18 slots with 7 features each . These dimensions are fixed so datasets can leverage the released pretraining weights. Order the features exactly as your YAML lists them (energy, pT , \u03b7 , \u03c6 , b-tag score, lepton flag, charge in the example). Use padding for missing particles and mask them out via x_mask . x_mask (N, 18) Boolean (or 0/1 ) mask indicating which particle slots in x correspond to real objects. Only entries with mask 1 contribute to losses and metrics. conditions (N, C) Event-level scalars. C is the number of global variables you define (10 in the example). You may add or remove variables as long as the order matches your YAML; if you do not supply any conditions, drop the key entirely. conditions_mask (N, 1) Mask for conditions . Set to 1 when the global features are present. If you omit conditions , omit this mask as well. \ud83c\udfaf Supervision Targets by Head \u00b6 Only provide the tensors required for the heads you enable in your training YAML. Omit unused targets or set them to empty arrays so the converter skips unnecessary storage. Classification Head \u00b6 Key Shape Meaning classification (N,) Process label per event. Combine with event_weight for weighted cross-entropy when sampling imbalanced campaigns. event_weight (N,) Optional per-event weight; defaults to 1 if omitted. Populate it alongside classification so the converter can broadcast the weights into the parquet shards. TruthGeneration Head \u00b6 Key Shape Meaning x_invisible (N, N_nu, F_nu) Invisible particle (e.g., neutrino) features. N_nu is the maximum number of invisible objects you intend to pad, 2 in the example, and F_nu is the number of features per invisible. Feature order is defined in your YAML under the TruthGeneration block. x_invisible_mask (N, N_nu) Flags which invisible entries are valid. num_invisible_raw (N,) Count of all invisible objects before quality cuts. num_invisible_valid (N,) Number of invisible objects associated with reconstructed parents. ReconGeneration Head \u00b6 ReconGeneration is self-supervised: it perturbs the visible point-cloud channels and learns to denoise them. The target specification (which channels to regenerate) lives directly in the YAML under the ReconGeneration configuration. No additional tensors beyond the standard inputs are required. Resonance Assignment Head \u00b6 Index conventions \u00b6 R = number of resonance targets per event (largest across your processes) D = maximum daughters any resonance can have (padding dimension) Child indices refer to rows in the fixed x tensor (18 slots). Use -1 for padding/null. Target Tensors \u00b6 Key Shape Meaning assignments-indices (N, R, D) Integer indices mapping each resonance target to its reconstructed daughters. Fill unused daughter slots with -1 . assignments-mask (N, R) Boolean mask indicating whether all required daughters for each resonance are successfully reconstructed. assignments-indices-mask (N, R, D) Per-daughter mask specifying which child indices are valid. 0 indicates padding for missing daughters. subprocess_id (N,) Integer label of the generating subprocess (Feynman diagram class). process_names (N,) String label of each subprocess. Must match the ordering in event_info.yaml and align with subprocess_id . Used only in preprocessing and producing normalization.pt . Quick Summary \u00b6 R = resonance targets, D = padded daughters Children index into the 18\u00d77 x slots; -1 = padding assignments-mask expects all daughters present; per-daughter gaps use assignments-indices-mask \ud83d\udcd0 Assignment internals: During conversion EveNet scans your assignment map to determine R and D , initialises arrays filled with -1 , and then writes the actual child indices along with boolean masks. The snippet below mirrors the loader logic so you can generate matching tensors in your own pipeline: full_indices = np . full (( num_events , n_targets , max_daughters ), - 1 , dtype = int ) full_mask = np . zeros (( num_events , n_targets ), dtype = bool ) index_mask = np . zeros (( num_events , n_targets , max_daughters ), dtype = bool ) # Fill with your resonance\u2192daughter mappings; mark valid entries in the masks. Segmentation Head \u00b6 Index conventions \u00b6 S = max number of resonance instances across all processes + 1 null instance (null at index S\u22121) Number of resonance tags + 1 null tag (null at index 0) Tensors are boolean except momentum. Target Tensors \u00b6 Key Shape Description segmentation-class (N, S, T) One-hot resonance tag for each resonance instance . Null tag = 0; null instance = S-1. segmentation-data (N, S, 18) Mask assigning input particles (18 dims) to each instance. Null instance = all zeros. segmentation-momentum (N, S, 4) True four-momentum (E,px,py,pz) for each instance; null instance = zeros. segmentation-full-class (N, S, T) 1 if instance is fully reconstructable for tag t ; else assigned to null tag. Quick Summary \u00b6 S = instances, T = classes Both include a null entry Boolean everywhere except 4-momentum Segmentation maps: instance \u2192 tag ( segmentation-class ) instance \u2192 particles ( segmentation-data ) instance \u2192 true 4-vector ( segmentation-momentum ) instance \u2192 fully-reconstructable flag ( segmentation-full-class ) Worked Input Example \u00b6 import numpy as np example = { \"x\" : np . zeros (( N , 18 , 7 ), dtype = np . float32 ), # fixed to (18, 7) for pretraining compatibility \"x_mask\" : np . zeros (( N , 18 ), dtype = bool ), # Optional globals \u2014 drop both keys if unused \"conditions\" : np . zeros (( N , C ), dtype = np . float32 ), \"conditions_mask\" : np . ones (( N , 1 ), dtype = bool ), # Classification head (weights default to ones if omitted) \"classification\" : np . zeros (( N ,), dtype = np . int32 ), \"event_weight\" : np . ones (( N ,), dtype = np . float32 ), # Head-specific entries sized by your resonance/segment definitions \"assignments-indices\" : np . full (( N , R , D ), - 1 , dtype = int ), \"assignments-mask\" : np . zeros (( N , R ), dtype = bool ), \"segmentation-data\" : np . zeros (( N , S , 18 ), dtype = bool ), # ... add heads you enabled ... } Feel free to adjust the head-specific dimensions ( R , D , S , T ) and the number of condition scalars C to match your physics process. The only fixed sizes are the point-cloud slots (18, 7) shared across datasets. Keep the YAML and the .npz dictionary in sync so the converter knows how many channels to expect and how to name them. \ud83d\udd04 NPZ \u2192 Parquet Conversion \u00b6 Assemble events into Python lists and save them with numpy.savez (or savez_compressed ). Each key listed above becomes an array inside the archive. Invoke the converter using the new dataloader-friendly CLI. Point to your event-info YAML via --config and choose whether you are supplying pre-split files or a combined set that should be split by the tool: # Explicit file-level splits python preprocessing/preprocess.py \\ --config share/event_info/pretrain.yaml \\ --train /path/to/train_*.npz \\ --val /path/to/val_*.npz \\ --test /path/to/test_*.npz \\ --store_dir /path/to/output # Event-level split with ratios (defaults to train only) python preprocessing/preprocess.py \\ --config share/event_info/pretrain.yaml \\ --files /path/to/combined_*.npz \\ --split_ratio 0 .8,0.1,0.1 \\ --store_dir /path/to/output Use --file instead of --files when processing a single archive. Set -v/--verbose to see detailed sanity-check output and saved file sizes. The converter reads the YAML to recover feature names, masks, and head activation flags, then emits: - train.parquet , val.parquet , and test.parquet containing flattened tensors (only for splits with data). - shape_metadata.json with the original shapes (e.g., (18, 7) for x ). - normalization.pt with channel-wise statistics and class weights computed from the training split. Inspect the logs. The script reports how many particles, invisible objects, and resonances were valid across the dataset\u2014helpful when debugging mask alignment. Statistics are aggregated only from the training data so validation and test remain untouched. \u2705 Runtime Checklist \u00b6 platform.data_parquet_dir points to the folder with the generated parquet shards and shape_metadata.json . options.Dataset.event_info references the same YAML ( share/event_info/pretrain.yaml or your copy). options.Dataset.normalization_file matches the normalization.pt produced during conversion. Only the heads you activated in the training YAML have matching supervision tensors in the parquet files. With those pieces in place, EveNet will rebuild the full event dictionary on the fly, apply the appropriate circular normalization for normalize_uniform channels, and route each tensor to the corresponding head.","title":"Data Preparation"},{"location":"data_preparation/#data-preparation-updated-input-reference","text":"This page documents the new preprocessing contract for EveNet. The reference schema in share/event_info/pretrain.yaml is bundled with the repository as an example, but you are free to adapt the feature lists to match your campaign. The goal of this guide is to help you build the .npz files that the EveNet converter ingests and to clarify how each tensor maps onto the model heads. \u26a0\ufe0f Ownership reminder: Users are responsible for generating the .npz files. EveNet does not reorder or reshape features for you\u2014the arrays must already follow the size and ordering implied by your event-info YAML. The converter simply validates the layout and writes the parquet outputs. Keep the YAML and the .npz dictionary in sync at all times. Config + CLI workflow Input tensor dictionary Supervision targets by head NPZ \u2192 Parquet conversion Runtime checklist","title":"\ud83e\uddea Data Preparation &amp; Updated Input Reference"},{"location":"data_preparation/#config-cli-workflow","text":"Start from an event-info YAML. The repository ships an example at share/event_info/pretrain.yaml ; copy or extend it to describe the objects, global variables, and heads you plan to enable. The display names inside the INPUTS block are just labels used for logging and plotting\u2014what matters is the order, which must match the tensor layout you write to disk. Produce an event dictionary. For every event, assemble a Python dictionary that satisfies the shapes described below and append it to the archive you will write to disk. When you call numpy.savez (or savez_compressed ), each key becomes an array with leading dimension N , the number of events in the file. Masks indicate which padded entries are valid and should contribute to the loss. Run the EveNet converter. Point preprocessing/preprocess.py at your .npz bundle and pass the matching YAML so the loader can recover feature names, the number of sequential vectors, and the heads you are enabling. The converter assumes both artifacts describe the same structure\u2014mismatches will surface as validation errors. Two ingest modes are supported: Explicit file splits : provide train/val/test files directly so the converter preserves your boundaries. On-the-fly splits : provide one or more unsplit files and let the converter perform an event-level split using your --split_ratio . Train or evaluate. Training configs reference the resulting parquet directory via platform.data_parquet_dir and reuse the same YAML in options.Dataset.event_info . \u2728 Normalization note. Features marked log_normalize in the YAML are converted with np.log1p during preprocessing (converter logs the affected columns). Values must be finite and non-negative or the run will fail so you can fix the upstream pipeline. Other tags ( normalize , none ) remain metadata only, while normalize_uniform still denotes the wrapped representation for circular variables ( \u03c6 ).","title":"\ud83d\udee0\ufe0f Config + CLI Workflow"},{"location":"data_preparation/#input-tensor-dictionary","text":"Each event is described by the following feature tensors. Shapes are shown with a leading N to indicate the number of stored events in a given .npz file. Masks share the same leading dimension as the value they gate. During conversion, preprocessing/sanity_checks.py logs these expectations and will cast dtypes when possible so you can quickly verify the layout coming from your pipeline. Key Shape Description num_vectors (N,) Total count of global + sequential objects per event. num_sequential_vectors (N,) Number of valid sequential entries per event. Mirrors num_vectors behaviour. x (N, 18, 7) Point-cloud tensor storing exactly 18 slots with 7 features each . These dimensions are fixed so datasets can leverage the released pretraining weights. Order the features exactly as your YAML lists them (energy, pT , \u03b7 , \u03c6 , b-tag score, lepton flag, charge in the example). Use padding for missing particles and mask them out via x_mask . x_mask (N, 18) Boolean (or 0/1 ) mask indicating which particle slots in x correspond to real objects. Only entries with mask 1 contribute to losses and metrics. conditions (N, C) Event-level scalars. C is the number of global variables you define (10 in the example). You may add or remove variables as long as the order matches your YAML; if you do not supply any conditions, drop the key entirely. conditions_mask (N, 1) Mask for conditions . Set to 1 when the global features are present. If you omit conditions , omit this mask as well.","title":"\ud83d\udce6 Input Tensor Dictionary"},{"location":"data_preparation/#supervision-targets-by-head","text":"Only provide the tensors required for the heads you enable in your training YAML. Omit unused targets or set them to empty arrays so the converter skips unnecessary storage.","title":"\ud83c\udfaf Supervision Targets by Head"},{"location":"data_preparation/#classification-head","text":"Key Shape Meaning classification (N,) Process label per event. Combine with event_weight for weighted cross-entropy when sampling imbalanced campaigns. event_weight (N,) Optional per-event weight; defaults to 1 if omitted. Populate it alongside classification so the converter can broadcast the weights into the parquet shards.","title":"Classification Head"},{"location":"data_preparation/#truthgeneration-head","text":"Key Shape Meaning x_invisible (N, N_nu, F_nu) Invisible particle (e.g., neutrino) features. N_nu is the maximum number of invisible objects you intend to pad, 2 in the example, and F_nu is the number of features per invisible. Feature order is defined in your YAML under the TruthGeneration block. x_invisible_mask (N, N_nu) Flags which invisible entries are valid. num_invisible_raw (N,) Count of all invisible objects before quality cuts. num_invisible_valid (N,) Number of invisible objects associated with reconstructed parents.","title":"TruthGeneration Head"},{"location":"data_preparation/#recongeneration-head","text":"ReconGeneration is self-supervised: it perturbs the visible point-cloud channels and learns to denoise them. The target specification (which channels to regenerate) lives directly in the YAML under the ReconGeneration configuration. No additional tensors beyond the standard inputs are required.","title":"ReconGeneration Head"},{"location":"data_preparation/#resonance-assignment-head","text":"","title":"Resonance Assignment Head"},{"location":"data_preparation/#index-conventions","text":"R = number of resonance targets per event (largest across your processes) D = maximum daughters any resonance can have (padding dimension) Child indices refer to rows in the fixed x tensor (18 slots). Use -1 for padding/null.","title":"Index conventions"},{"location":"data_preparation/#target-tensors","text":"Key Shape Meaning assignments-indices (N, R, D) Integer indices mapping each resonance target to its reconstructed daughters. Fill unused daughter slots with -1 . assignments-mask (N, R) Boolean mask indicating whether all required daughters for each resonance are successfully reconstructed. assignments-indices-mask (N, R, D) Per-daughter mask specifying which child indices are valid. 0 indicates padding for missing daughters. subprocess_id (N,) Integer label of the generating subprocess (Feynman diagram class). process_names (N,) String label of each subprocess. Must match the ordering in event_info.yaml and align with subprocess_id . Used only in preprocessing and producing normalization.pt .","title":"Target Tensors"},{"location":"data_preparation/#quick-summary","text":"R = resonance targets, D = padded daughters Children index into the 18\u00d77 x slots; -1 = padding assignments-mask expects all daughters present; per-daughter gaps use assignments-indices-mask \ud83d\udcd0 Assignment internals: During conversion EveNet scans your assignment map to determine R and D , initialises arrays filled with -1 , and then writes the actual child indices along with boolean masks. The snippet below mirrors the loader logic so you can generate matching tensors in your own pipeline: full_indices = np . full (( num_events , n_targets , max_daughters ), - 1 , dtype = int ) full_mask = np . zeros (( num_events , n_targets ), dtype = bool ) index_mask = np . zeros (( num_events , n_targets , max_daughters ), dtype = bool ) # Fill with your resonance\u2192daughter mappings; mark valid entries in the masks.","title":"Quick Summary"},{"location":"data_preparation/#segmentation-head","text":"","title":"Segmentation Head"},{"location":"data_preparation/#index-conventions_1","text":"S = max number of resonance instances across all processes + 1 null instance (null at index S\u22121) Number of resonance tags + 1 null tag (null at index 0) Tensors are boolean except momentum.","title":"Index conventions"},{"location":"data_preparation/#target-tensors_1","text":"Key Shape Description segmentation-class (N, S, T) One-hot resonance tag for each resonance instance . Null tag = 0; null instance = S-1. segmentation-data (N, S, 18) Mask assigning input particles (18 dims) to each instance. Null instance = all zeros. segmentation-momentum (N, S, 4) True four-momentum (E,px,py,pz) for each instance; null instance = zeros. segmentation-full-class (N, S, T) 1 if instance is fully reconstructable for tag t ; else assigned to null tag.","title":"Target Tensors"},{"location":"data_preparation/#quick-summary_1","text":"S = instances, T = classes Both include a null entry Boolean everywhere except 4-momentum Segmentation maps: instance \u2192 tag ( segmentation-class ) instance \u2192 particles ( segmentation-data ) instance \u2192 true 4-vector ( segmentation-momentum ) instance \u2192 fully-reconstructable flag ( segmentation-full-class )","title":"Quick Summary"},{"location":"data_preparation/#worked-input-example","text":"import numpy as np example = { \"x\" : np . zeros (( N , 18 , 7 ), dtype = np . float32 ), # fixed to (18, 7) for pretraining compatibility \"x_mask\" : np . zeros (( N , 18 ), dtype = bool ), # Optional globals \u2014 drop both keys if unused \"conditions\" : np . zeros (( N , C ), dtype = np . float32 ), \"conditions_mask\" : np . ones (( N , 1 ), dtype = bool ), # Classification head (weights default to ones if omitted) \"classification\" : np . zeros (( N ,), dtype = np . int32 ), \"event_weight\" : np . ones (( N ,), dtype = np . float32 ), # Head-specific entries sized by your resonance/segment definitions \"assignments-indices\" : np . full (( N , R , D ), - 1 , dtype = int ), \"assignments-mask\" : np . zeros (( N , R ), dtype = bool ), \"segmentation-data\" : np . zeros (( N , S , 18 ), dtype = bool ), # ... add heads you enabled ... } Feel free to adjust the head-specific dimensions ( R , D , S , T ) and the number of condition scalars C to match your physics process. The only fixed sizes are the point-cloud slots (18, 7) shared across datasets. Keep the YAML and the .npz dictionary in sync so the converter knows how many channels to expect and how to name them.","title":"Worked Input Example"},{"location":"data_preparation/#npz-parquet-conversion","text":"Assemble events into Python lists and save them with numpy.savez (or savez_compressed ). Each key listed above becomes an array inside the archive. Invoke the converter using the new dataloader-friendly CLI. Point to your event-info YAML via --config and choose whether you are supplying pre-split files or a combined set that should be split by the tool: # Explicit file-level splits python preprocessing/preprocess.py \\ --config share/event_info/pretrain.yaml \\ --train /path/to/train_*.npz \\ --val /path/to/val_*.npz \\ --test /path/to/test_*.npz \\ --store_dir /path/to/output # Event-level split with ratios (defaults to train only) python preprocessing/preprocess.py \\ --config share/event_info/pretrain.yaml \\ --files /path/to/combined_*.npz \\ --split_ratio 0 .8,0.1,0.1 \\ --store_dir /path/to/output Use --file instead of --files when processing a single archive. Set -v/--verbose to see detailed sanity-check output and saved file sizes. The converter reads the YAML to recover feature names, masks, and head activation flags, then emits: - train.parquet , val.parquet , and test.parquet containing flattened tensors (only for splits with data). - shape_metadata.json with the original shapes (e.g., (18, 7) for x ). - normalization.pt with channel-wise statistics and class weights computed from the training split. Inspect the logs. The script reports how many particles, invisible objects, and resonances were valid across the dataset\u2014helpful when debugging mask alignment. Statistics are aggregated only from the training data so validation and test remain untouched.","title":"\ud83d\udd04 NPZ \u2192 Parquet Conversion"},{"location":"data_preparation/#runtime-checklist","text":"platform.data_parquet_dir points to the folder with the generated parquet shards and shape_metadata.json . options.Dataset.event_info references the same YAML ( share/event_info/pretrain.yaml or your copy). options.Dataset.normalization_file matches the normalization.pt produced during conversion. Only the heads you activated in the training YAML have matching supervision tensors in the parquet files. With those pieces in place, EveNet will rebuild the full event dictionary on the fly, apply the appropriate circular normalization for normalize_uniform channels, and route each tensor to the corresponding head.","title":"\u2705 Runtime Checklist"},{"location":"getting_started/","text":"\ud83d\udcd8 EveNet Tutorial: From Zero to First Predictions \u00b6 New to EveNet? This tutorial walks you through the end-to-end workflow so you can move from a clean checkout to model predictions with confidence. Each step links to a deeper reference guide if you need more detail. 1. Choose Your Setup Path \u00b6 Option A \u2014 Quick Start (Docker + PyPI) \u00b6 Ideal when you want the official binaries and ready-made CLIs without touching the source code. Pull the runtime image that bundles CUDA, PyTorch, Ray, and common utilities. docker pull docker.io/avencast1994/evenet:1.5 docker run --gpus all -it \\ -v /path/to/your/data:/workspace/data \\ docker.io/avencast1994/evenet:1.5 Inside the container (or any GPU-ready Python 3.12+ environment), install EveNet from PyPI. pip install evenet Invoke the packaged CLIs with your configuration files. evenet-train share/finetune-example.yaml --ray_dir ~/ray_results evenet-predict share/predict-example.yaml This path is \u201cplug and play\u201d\u2014you only manage YAML configs and data paths. Option B \u2014 Advanced (Source Checkout) \u00b6 Choose this when you want to edit the Lightning modules, extend datasets, or customize the CLI behavior. Clone the repository (or mount it inside the Docker image from Option A). git clone https://github.com/UW-ePE-ML/EveNet_Public.git cd EveNet_Public Reuse the provided Docker image or create your own environment on Python 3.12+. Docker: bind mount your checkout and data into the container so code changes persist. Native install: pip install -r requirements.txt (plus any CUDA/PyTorch builds required by your system). Run the CLIs straight from source when iterating rapidly. python -m evenet.train share/finetune-example.yaml --ray_dir ~/ray_results python -m evenet.predict share/predict-example.yaml # or call the scripts directly python evenet/train.py share/finetune-example.yaml --ray_dir ~/ray_results python evenet/predict.py share/predict-example.yaml Both options are interoperable\u2014you can install the PyPI package for quick tests and then switch to the cloned source for deeper development. 2. Understand the Project Layout (Advanced Users) \u00b6 Before running any commands, skim these key directories: evenet/ \u2013 PyTorch Lightning modules, Ray data pipelines, and trainer utilities. share/ \u2013 Ready-to-edit YAML configurations for fine-tuning and prediction. docs/ \u2013 Reference documentation that expands on this tutorial. Start with Model Architecture Tour to see how point-cloud and global features flow through EveNet. downstreams/ \u2013 Example downstream analysis scripts built on top of EveNet outputs. 3. Verify Your Environment \u00b6 Review cluster helpers as needed. The Docker/ and NERSC/ directories include recipes and SLURM launch scripts tailored for HPC environments. Confirm GPU visibility (if available). python -c \"import torch; print(torch.cuda.device_count())\" 4. Download Pretrained Weights \u00b6 EveNet is released as a pretrained foundation model. Start from these weights when fine-tuning or making predictions. Browse and download weights directly from HuggingFace: \ud83d\udc49 Avencast/EveNet on HuggingFace Place the downloaded .ckpt file somewhere accessible and update your YAML configs with the path (see below). 5. Prepare Input Data \u00b6 Do not run the scripts under preprocessing/ , which are only for large-scale pretraining. For fine-tuning, prepare your own dataset in the EveNet format. You are responsible for converting your physics ntuples into the EveNet parquet + metadata format. See data preparation guide for details on schema, normalization, and writer options. \ud83d\udd0d Tip: Keep your preprocessing configs in version control so you can reproduce and document each dataset. 6. Configure an Experiment \u00b6 Note: The example configs are not standalone . Each one uses default: ...yaml to load additional base configs. The parser resolves these paths relative to the example\u2019s location, so you must also copy the referenced YAML files and preserve their directory structure. Copy both share/finetune-example.yaml (for training) and share/predict-example.yaml (for inference) into your working directory. Update key fields for your experiment: platform.data_parquet_dir \u2192 directory of your processed parquet files Dataset.normalization_file \u2192 path to the normalization statistics you created options.Training.pretrain_model_load_path \u2192 pretrained checkpoint to load logger \u2192 project name, WANDB API key, or local log directory For a detailed description of every section and all available overrides, see the configuration reference . 7. Fine-Tune the Model \u00b6 Export your Weights & Biases API key if you plan to log online. export WANDB_API_KEY = <your_key> Launch training with your updated YAML. Quick start users: run the packaged CLI after pip install evenet . evenet-train path/to/your-train-config.yaml Source checkout: execute the module directly to pick up local code edits. python -m evenet.train path/to/your-train-config.yaml Monitor progress: Console output provides per-epoch metrics and checkpoint locations. WANDB dashboards (if enabled) visualize loss curves and system stats. Checkpoints and logs are stored under options.Training.model_checkpoint_save_path . 8. Generate Predictions \u00b6 Ensure the prediction YAML points to your trained (or pretrained) checkpoint via options.Training.model_checkpoint_load_path . Launch inference with either interface. # PyPI package evenet-predict path/to/your-predict-config.yaml # Source checkout python -m evenet.predict path/to/your-predict-config.yaml Outputs land in the configured writers (e.g., parquet, numpy archives). See docs/predict.md for writer options and schema notes. 9. Explore and Iterate \u00b6 Use the artifacts written in the prediction step for downstream analysis (examples live under downstreams/ ). Adjust YAML hyperparameters, architecture templates, or preprocessing selections and repeat the workflow. When adding new datasets or modules, contribute documentation updates so the next user can follow your path. Happy exploring! \ud83d\ude80","title":"Getting Started"},{"location":"getting_started/#evenet-tutorial-from-zero-to-first-predictions","text":"New to EveNet? This tutorial walks you through the end-to-end workflow so you can move from a clean checkout to model predictions with confidence. Each step links to a deeper reference guide if you need more detail.","title":"\ud83d\udcd8 EveNet Tutorial: From Zero to First Predictions"},{"location":"getting_started/#1-choose-your-setup-path","text":"","title":"1. Choose Your Setup Path"},{"location":"getting_started/#option-a-quick-start-docker-pypi","text":"Ideal when you want the official binaries and ready-made CLIs without touching the source code. Pull the runtime image that bundles CUDA, PyTorch, Ray, and common utilities. docker pull docker.io/avencast1994/evenet:1.5 docker run --gpus all -it \\ -v /path/to/your/data:/workspace/data \\ docker.io/avencast1994/evenet:1.5 Inside the container (or any GPU-ready Python 3.12+ environment), install EveNet from PyPI. pip install evenet Invoke the packaged CLIs with your configuration files. evenet-train share/finetune-example.yaml --ray_dir ~/ray_results evenet-predict share/predict-example.yaml This path is \u201cplug and play\u201d\u2014you only manage YAML configs and data paths.","title":"Option A \u2014 Quick Start (Docker + PyPI)"},{"location":"getting_started/#option-b-advanced-source-checkout","text":"Choose this when you want to edit the Lightning modules, extend datasets, or customize the CLI behavior. Clone the repository (or mount it inside the Docker image from Option A). git clone https://github.com/UW-ePE-ML/EveNet_Public.git cd EveNet_Public Reuse the provided Docker image or create your own environment on Python 3.12+. Docker: bind mount your checkout and data into the container so code changes persist. Native install: pip install -r requirements.txt (plus any CUDA/PyTorch builds required by your system). Run the CLIs straight from source when iterating rapidly. python -m evenet.train share/finetune-example.yaml --ray_dir ~/ray_results python -m evenet.predict share/predict-example.yaml # or call the scripts directly python evenet/train.py share/finetune-example.yaml --ray_dir ~/ray_results python evenet/predict.py share/predict-example.yaml Both options are interoperable\u2014you can install the PyPI package for quick tests and then switch to the cloned source for deeper development.","title":"Option B \u2014 Advanced (Source Checkout)"},{"location":"getting_started/#2-understand-the-project-layout-advanced-users","text":"Before running any commands, skim these key directories: evenet/ \u2013 PyTorch Lightning modules, Ray data pipelines, and trainer utilities. share/ \u2013 Ready-to-edit YAML configurations for fine-tuning and prediction. docs/ \u2013 Reference documentation that expands on this tutorial. Start with Model Architecture Tour to see how point-cloud and global features flow through EveNet. downstreams/ \u2013 Example downstream analysis scripts built on top of EveNet outputs.","title":"2. Understand the Project Layout (Advanced Users)"},{"location":"getting_started/#3-verify-your-environment","text":"Review cluster helpers as needed. The Docker/ and NERSC/ directories include recipes and SLURM launch scripts tailored for HPC environments. Confirm GPU visibility (if available). python -c \"import torch; print(torch.cuda.device_count())\"","title":"3. Verify Your Environment"},{"location":"getting_started/#4-download-pretrained-weights","text":"EveNet is released as a pretrained foundation model. Start from these weights when fine-tuning or making predictions. Browse and download weights directly from HuggingFace: \ud83d\udc49 Avencast/EveNet on HuggingFace Place the downloaded .ckpt file somewhere accessible and update your YAML configs with the path (see below).","title":"4. Download Pretrained Weights"},{"location":"getting_started/#5-prepare-input-data","text":"Do not run the scripts under preprocessing/ , which are only for large-scale pretraining. For fine-tuning, prepare your own dataset in the EveNet format. You are responsible for converting your physics ntuples into the EveNet parquet + metadata format. See data preparation guide for details on schema, normalization, and writer options. \ud83d\udd0d Tip: Keep your preprocessing configs in version control so you can reproduce and document each dataset.","title":"5. Prepare Input Data"},{"location":"getting_started/#6-configure-an-experiment","text":"Note: The example configs are not standalone . Each one uses default: ...yaml to load additional base configs. The parser resolves these paths relative to the example\u2019s location, so you must also copy the referenced YAML files and preserve their directory structure. Copy both share/finetune-example.yaml (for training) and share/predict-example.yaml (for inference) into your working directory. Update key fields for your experiment: platform.data_parquet_dir \u2192 directory of your processed parquet files Dataset.normalization_file \u2192 path to the normalization statistics you created options.Training.pretrain_model_load_path \u2192 pretrained checkpoint to load logger \u2192 project name, WANDB API key, or local log directory For a detailed description of every section and all available overrides, see the configuration reference .","title":"6. Configure an Experiment"},{"location":"getting_started/#7-fine-tune-the-model","text":"Export your Weights & Biases API key if you plan to log online. export WANDB_API_KEY = <your_key> Launch training with your updated YAML. Quick start users: run the packaged CLI after pip install evenet . evenet-train path/to/your-train-config.yaml Source checkout: execute the module directly to pick up local code edits. python -m evenet.train path/to/your-train-config.yaml Monitor progress: Console output provides per-epoch metrics and checkpoint locations. WANDB dashboards (if enabled) visualize loss curves and system stats. Checkpoints and logs are stored under options.Training.model_checkpoint_save_path .","title":"7. Fine-Tune the Model"},{"location":"getting_started/#8-generate-predictions","text":"Ensure the prediction YAML points to your trained (or pretrained) checkpoint via options.Training.model_checkpoint_load_path . Launch inference with either interface. # PyPI package evenet-predict path/to/your-predict-config.yaml # Source checkout python -m evenet.predict path/to/your-predict-config.yaml Outputs land in the configured writers (e.g., parquet, numpy archives). See docs/predict.md for writer options and schema notes.","title":"8. Generate Predictions"},{"location":"getting_started/#9-explore-and-iterate","text":"Use the artifacts written in the prediction step for downstream analysis (examples live under downstreams/ ). Adjust YAML hyperparameters, architecture templates, or preprocessing selections and repeat the workflow. When adding new datasets or modules, contribute documentation updates so the next user can follow your path. Happy exploring! \ud83d\ude80","title":"9. Explore and Iterate"},{"location":"model_architecture/","text":"\ud83e\udde0 Model Architecture Tour \u00b6 Take a guided walk through EveNet\u2019s multitask architecture\u2014from input normalization to the specialized heads. Pair this with the configuration guide to see how YAML choices shape each component. Signal flow at a glance Input normalization Shared body Task heads Progressive training hooks Customizing the network \ud83d\udd01 Signal Flow at a Glance \u00b6 Inputs are split into point-cloud tensors and global per-event features. Explicit supervision arrives through the target nodes shown on the right, while the reconstruction head trains against the noisy point-cloud inputs it perturbs. Each module in the flow is instantiated inside evenet/network/evenet_model.py using the options loaded from your YAML files. The optional global-diffusion network is configured separately and conditions on the same normalized scalars. \ud83e\uddf4 Input Normalization \u00b6 When EveNetModel is built, it grabs feature statistics from normalization.pt plus schema details from event_info and constructs a collection of Normalizer layers: Sequential features ( INPUTS/Source ) use a Normalizer that understands mixed discrete/continuous distributions and optional inverse-CDF indices. Global features ( INPUTS/Conditions ) map through a second Normalizer sized to the event-level vector. Multiplicity channels ( num_vectors , num_sequential_vectors ) are normalized when generation heads are active. Invisible particle targets reuse the sequential embedding width and are padded to max_neutrinos so diffusion heads can operate consistently whenever truth-level supervision is available. Implementation details live near the top of evenet/network/evenet_model.py . \ud83e\uddf1 Shared Body \u00b6 \ud83c\udf10 Global Embedding \u00b6 GlobalVectorEmbedding converts the condition vector into learned tokens. Hyperparameters like depth, hidden dimension, dropout, and activation come from the Body.GlobalEmbedding block described in the configuration reference . \ud83e\uddf2 PET Body \u00b6 PETBody processes the sequential particle cloud with transformer-style layers, local neighborhood attention, and optional stochastic feature dropping. Configure num_layers , num_heads , feature_drop , and local_point_index under Body.PET in your network block (see configuration reference ). \ud83e\uddf5 Object Encoder \u00b6 Outputs from the PET body and global tokens meet in the ObjectEncoder , which mixes information across objects. Attention depth, head counts, positional embedding size, and skip connections are controlled by Body.ObjectEncoder (see configuration reference ). \ud83c\udfaf Task Heads \u00b6 Heads are instantiated only when options.Training.Components.<Head>.include is true . EveNet groups them into discriminative predictors that score events and objects, and generative heads that either reconstruct their own inputs or learn diffusion processes against explicit supervision. \ud83d\udd0d Discriminative Heads \u00b6 \ud83c\udff7\ufe0f Classification \u00b6 Predicts process probabilities using ClassificationHead . Configure layer counts, hidden size, dropout, and optional attention under Classification in the network YAML (see configuration reference ). \ud83d\udcc8 Regression \u00b6 RegressionHead regresses continuous targets (momenta, masses). Normalization tensors ( regression_mean , regression_std ) are injected so outputs can be de-standardized. Hyperparameters mirror the classification head (see configuration reference ). \ud83d\udd17 Assignment \u00b6 SharedAssignmentHead tackles combinatorial matching between reconstructed objects and truth daughters defined in event_info . It leverages symmetry-aware attention and optional detection layers. Tune feature_drop , attention heads, and decoder depth via the Assignment block (see configuration reference ). \ud83c\udf08 Segmentation \u00b6 SegmentationHead predicts binary masks for resonance-specific particle groups. Configure the number of queries, transformer layers, and projection widths in the Segmentation block (see configuration reference ). \ud83c\udf2c\ufe0f Generative Heads \u00b6 Two sibling diffusion heads branch off from the shared body, each with a distinct training objective, plus an optional standalone module for scalar generation: Head Objective type Input features Supervision Notes ReconGeneration Self-supervised reconstruction PET embeddings + global tokens before the object encoder. No external targets\u2014the head denoises the noisy visible point-cloud channels it perturbs. Shares the PET backbone directly so reconstruction quality reflects the sequential encoder capacity. Configure under ReconGeneration . TruthGeneration Supervised generation PET embeddings + global tokens with optional invisible padding. Padded invisible particle features (e.g., neutrinos) supplied in the dataset. Mirrors the reconstruction architecture but learns to sample toward truth-level targets. Settings sit under TruthGeneration . \ud83d\udca1 GlobalGeneration remains available as an independent diffusion network for event-level scalars. It conditions on the normalized global tokens but does not connect through the PET/ObjectEncoder stack, so you can enable or disable it without affecting the primary generative heads. \ud83c\udf00 Progressive Training Hooks \u00b6 EveNetModel exposes schedule_flags describing which heads are active (diffusion, neutrino, deterministic). The training loop combines these flags with the curriculum defined in options.ProgressiveTraining so that loss weights, dropout, EMA decay, or teacher-forcing gradually adjust across stages. Inspect the scheduling logic in evenet/network/evenet_model.py and pair it with the YAML stages summarized in the configuration reference . \ud83d\udee0\ufe0f Customizing the Network \u00b6 Pick a template \u2013 choose a network block described in the configuration reference and copy it into your experiment YAML. Override selectively \u2013 in your top-level YAML, override only the fields you want to tweak (e.g., set Body.PET.feature_drop: 0.0 for fine-tuning). Match supervision \u2013 enable heads under options.Training.Components only when the dataset provides the required targets. Refresh normalization \u2013 if you change the input schema in event_info , rerun preprocessing so new statistics land in normalization.pt (see the saving logic in preprocessing/postprocessor.py ). With these controls, you can resize EveNet for tiny studies or scale it up for production campaigns\u2014all while keeping the codepath consistent.","title":"Model Architecture"},{"location":"model_architecture/#model-architecture-tour","text":"Take a guided walk through EveNet\u2019s multitask architecture\u2014from input normalization to the specialized heads. Pair this with the configuration guide to see how YAML choices shape each component. Signal flow at a glance Input normalization Shared body Task heads Progressive training hooks Customizing the network","title":"\ud83e\udde0 Model Architecture Tour"},{"location":"model_architecture/#signal-flow-at-a-glance","text":"Inputs are split into point-cloud tensors and global per-event features. Explicit supervision arrives through the target nodes shown on the right, while the reconstruction head trains against the noisy point-cloud inputs it perturbs. Each module in the flow is instantiated inside evenet/network/evenet_model.py using the options loaded from your YAML files. The optional global-diffusion network is configured separately and conditions on the same normalized scalars.","title":"\ud83d\udd01 Signal Flow at a Glance"},{"location":"model_architecture/#input-normalization","text":"When EveNetModel is built, it grabs feature statistics from normalization.pt plus schema details from event_info and constructs a collection of Normalizer layers: Sequential features ( INPUTS/Source ) use a Normalizer that understands mixed discrete/continuous distributions and optional inverse-CDF indices. Global features ( INPUTS/Conditions ) map through a second Normalizer sized to the event-level vector. Multiplicity channels ( num_vectors , num_sequential_vectors ) are normalized when generation heads are active. Invisible particle targets reuse the sequential embedding width and are padded to max_neutrinos so diffusion heads can operate consistently whenever truth-level supervision is available. Implementation details live near the top of evenet/network/evenet_model.py .","title":"\ud83e\uddf4 Input Normalization"},{"location":"model_architecture/#shared-body","text":"","title":"\ud83e\uddf1 Shared Body"},{"location":"model_architecture/#global-embedding","text":"GlobalVectorEmbedding converts the condition vector into learned tokens. Hyperparameters like depth, hidden dimension, dropout, and activation come from the Body.GlobalEmbedding block described in the configuration reference .","title":"\ud83c\udf10 Global Embedding"},{"location":"model_architecture/#pet-body","text":"PETBody processes the sequential particle cloud with transformer-style layers, local neighborhood attention, and optional stochastic feature dropping. Configure num_layers , num_heads , feature_drop , and local_point_index under Body.PET in your network block (see configuration reference ).","title":"\ud83e\uddf2 PET Body"},{"location":"model_architecture/#object-encoder","text":"Outputs from the PET body and global tokens meet in the ObjectEncoder , which mixes information across objects. Attention depth, head counts, positional embedding size, and skip connections are controlled by Body.ObjectEncoder (see configuration reference ).","title":"\ud83e\uddf5 Object Encoder"},{"location":"model_architecture/#task-heads","text":"Heads are instantiated only when options.Training.Components.<Head>.include is true . EveNet groups them into discriminative predictors that score events and objects, and generative heads that either reconstruct their own inputs or learn diffusion processes against explicit supervision.","title":"\ud83c\udfaf Task Heads"},{"location":"model_architecture/#discriminative-heads","text":"","title":"\ud83d\udd0d Discriminative Heads"},{"location":"model_architecture/#classification","text":"Predicts process probabilities using ClassificationHead . Configure layer counts, hidden size, dropout, and optional attention under Classification in the network YAML (see configuration reference ).","title":"\ud83c\udff7\ufe0f Classification"},{"location":"model_architecture/#regression","text":"RegressionHead regresses continuous targets (momenta, masses). Normalization tensors ( regression_mean , regression_std ) are injected so outputs can be de-standardized. Hyperparameters mirror the classification head (see configuration reference ).","title":"\ud83d\udcc8 Regression"},{"location":"model_architecture/#assignment","text":"SharedAssignmentHead tackles combinatorial matching between reconstructed objects and truth daughters defined in event_info . It leverages symmetry-aware attention and optional detection layers. Tune feature_drop , attention heads, and decoder depth via the Assignment block (see configuration reference ).","title":"\ud83d\udd17 Assignment"},{"location":"model_architecture/#segmentation","text":"SegmentationHead predicts binary masks for resonance-specific particle groups. Configure the number of queries, transformer layers, and projection widths in the Segmentation block (see configuration reference ).","title":"\ud83c\udf08 Segmentation"},{"location":"model_architecture/#generative-heads","text":"Two sibling diffusion heads branch off from the shared body, each with a distinct training objective, plus an optional standalone module for scalar generation: Head Objective type Input features Supervision Notes ReconGeneration Self-supervised reconstruction PET embeddings + global tokens before the object encoder. No external targets\u2014the head denoises the noisy visible point-cloud channels it perturbs. Shares the PET backbone directly so reconstruction quality reflects the sequential encoder capacity. Configure under ReconGeneration . TruthGeneration Supervised generation PET embeddings + global tokens with optional invisible padding. Padded invisible particle features (e.g., neutrinos) supplied in the dataset. Mirrors the reconstruction architecture but learns to sample toward truth-level targets. Settings sit under TruthGeneration . \ud83d\udca1 GlobalGeneration remains available as an independent diffusion network for event-level scalars. It conditions on the normalized global tokens but does not connect through the PET/ObjectEncoder stack, so you can enable or disable it without affecting the primary generative heads.","title":"\ud83c\udf2c\ufe0f Generative Heads"},{"location":"model_architecture/#progressive-training-hooks","text":"EveNetModel exposes schedule_flags describing which heads are active (diffusion, neutrino, deterministic). The training loop combines these flags with the curriculum defined in options.ProgressiveTraining so that loss weights, dropout, EMA decay, or teacher-forcing gradually adjust across stages. Inspect the scheduling logic in evenet/network/evenet_model.py and pair it with the YAML stages summarized in the configuration reference .","title":"\ud83c\udf00 Progressive Training Hooks"},{"location":"model_architecture/#customizing-the-network","text":"Pick a template \u2013 choose a network block described in the configuration reference and copy it into your experiment YAML. Override selectively \u2013 in your top-level YAML, override only the fields you want to tweak (e.g., set Body.PET.feature_drop: 0.0 for fine-tuning). Match supervision \u2013 enable heads under options.Training.Components only when the dataset provides the required targets. Refresh normalization \u2013 if you change the input schema in event_info , rerun preprocessing so new statistics land in normalization.pt (see the saving logic in preprocessing/postprocessor.py ). With these controls, you can resize EveNet for tiny studies or scale it up for production campaigns\u2014all while keeping the codepath consistent.","title":"\ud83d\udee0\ufe0f Customizing the Network"},{"location":"predict/","text":"Prediction \u00b6 \ud83d\ude80 Quickstart: Install the package with pip install evenet and launch inference via evenet-predict <config.yaml> . Working from the cloned repo? Run python -m evenet.predict <config.yaml> instead.","title":"Prediction"},{"location":"predict/#prediction","text":"\ud83d\ude80 Quickstart: Install the package with pip install evenet and launch inference via evenet-predict <config.yaml> . Working from the cloned repo? Run python -m evenet.predict <config.yaml> instead.","title":"Prediction"},{"location":"preprocess_internal_only/","text":"\ud83e\uddea Data Preprocessing for EveNet \u00b6 This preprocessing pipeline prepares structured event-level datasets for EveNet. It merges input .h5 files, flattens structured arrays into 2D pyarrow tables, calculates normalization statistics, and saves outputs as .parquet and .json files ready for model training. \ud83d\udccd Folder Structure \u00b6 The input directory should follow this structure: /global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data/ \u251c\u2500\u2500 Run_2.Dec20/ \u2502 \u251c\u2500\u2500 run_1/ \u2502 \u2502 \u251c\u2500\u2500 process_1.h5 \u2502 \u2502 \u251c\u2500\u2500 process_2.h5 \u2502 \u251c\u2500\u2500 run_2/ \u2502 \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 Run_2.Dec21/ \u2502 \u251c\u2500\u2500 run_1/ \u2502 \u2502 \u251c\u2500\u2500 ... ... Each Run folder (e.g., Run_2.Dec20) contains sub-run folders (run_1, run_2, \u2026), which hold the .h5 files representing different processes. \ud83d\udee0 What the Script Does \u00b6 For every sub-run folder: 1. Merges all .h5 process files into a single structured data dictionary. 2. Converts structured data into a 2D pyarrow Table. 3. Saves the output as: - data_ .parquet (flattened table) - shape_metadata.json (used for reconstruction) 4. Computes normalization statistics (mean, std) across all data. 5. Merges statistics from all sub-runs and saves a single .pt normalization file. \ud83d\ude80 Run on NERSC \u00b6 Paths Input root: /global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data Pretrain Run folders: Run_2.Dec20 , Run_2.Dec21 , Run_2.Dec22 Output directory: better to say on $SCRATCH folder Example Command pretrain_dir = \"/global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data\" output_dir = \" $PSCRATCH /Event_Level_Analysis/Pretrain_Parquet/\" python3 preprocessing/preprocess.py share/preprocess_pretrain.yaml \\ --c 25 \\ --pretrain_dirs \\ ${ pretrain_dir } /Run_2.Dec20 \\ ${ pretrain_dir } /Run_2.Dec21 \\ ${ pretrain_dir } /Run_2.Dec22 \\ ${ pretrain_dir } /Run_2.Pretrain.20250505 \\ ${ pretrain_dir } /Run_2.Pretrain.20250507 \\ ${ pretrain_dir } /Run_3.Pretrain.20250527.Run2Extra \\ --store_dir ${ output_dir } /run.20250527.654M | tee ${ output_dir } /run.20250527.654M.log shifter python3 preprocessing/preprocess.py share/preprocess_pretrain.yaml \\ --c 60 \\ --pretrain_dirs \\ ${ pretrain_dir } /Combined_Balanced \\ --store_dir ${ output_dir } /run.20250625.2700M This command will: - Process all sub-runs in the three Run folders - Generate flattened .parquet files and normalization .pt file - Save logs to run.20250403.log Global Shuffle python3 NERSC/global_shuffle.py --first_shuffle_percent 1 .0 --second_shuffle_percent 0 .5 25 \\ /pscratch/sd/a/avencast/Event_Level_Analysis/Pretrain_Parquet/run.20250527.654M/ \ud83d\udce4 Output Files \u00b6 Under the output directory ( --store_dir ), the script saves: /run.20250403/ \u251c\u2500\u2500 data_<timestamp>.parquet # Flattened input table \u251c\u2500\u2500 shape_metadata.json # Shape metadata for reconstruction \u251c\u2500\u2500 regression_norm.pt # Normalization statistics \u2514\u2500\u2500 run.20250403.log # Optional log if tee is used \u26a0\ufe0f Notes \u00b6 You must supply a valid preprocess_config.yaml (e.g., EveNet/share/preprocess_pretrain.yaml ). The script will skip any sub-run folder where no valid .h5 files are found or matching fails. All normalization is computed globally across all sub-runs and merged at the end.","title":"Internal Notes"},{"location":"preprocess_internal_only/#data-preprocessing-for-evenet","text":"This preprocessing pipeline prepares structured event-level datasets for EveNet. It merges input .h5 files, flattens structured arrays into 2D pyarrow tables, calculates normalization statistics, and saves outputs as .parquet and .json files ready for model training.","title":"\ud83e\uddea Data Preprocessing for EveNet"},{"location":"preprocess_internal_only/#folder-structure","text":"The input directory should follow this structure: /global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data/ \u251c\u2500\u2500 Run_2.Dec20/ \u2502 \u251c\u2500\u2500 run_1/ \u2502 \u2502 \u251c\u2500\u2500 process_1.h5 \u2502 \u2502 \u251c\u2500\u2500 process_2.h5 \u2502 \u251c\u2500\u2500 run_2/ \u2502 \u2502 \u251c\u2500\u2500 ... \u251c\u2500\u2500 Run_2.Dec21/ \u2502 \u251c\u2500\u2500 run_1/ \u2502 \u2502 \u251c\u2500\u2500 ... ... Each Run folder (e.g., Run_2.Dec20) contains sub-run folders (run_1, run_2, \u2026), which hold the .h5 files representing different processes.","title":"\ud83d\udccd Folder Structure"},{"location":"preprocess_internal_only/#what-the-script-does","text":"For every sub-run folder: 1. Merges all .h5 process files into a single structured data dictionary. 2. Converts structured data into a 2D pyarrow Table. 3. Saves the output as: - data_ .parquet (flattened table) - shape_metadata.json (used for reconstruction) 4. Computes normalization statistics (mean, std) across all data. 5. Merges statistics from all sub-runs and saves a single .pt normalization file.","title":"\ud83d\udee0 What the Script Does"},{"location":"preprocess_internal_only/#run-on-nersc","text":"Paths Input root: /global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data Pretrain Run folders: Run_2.Dec20 , Run_2.Dec21 , Run_2.Dec22 Output directory: better to say on $SCRATCH folder Example Command pretrain_dir = \"/global/cfs/cdirs/m2616/avencast/Event_Level_Analysis/data\" output_dir = \" $PSCRATCH /Event_Level_Analysis/Pretrain_Parquet/\" python3 preprocessing/preprocess.py share/preprocess_pretrain.yaml \\ --c 25 \\ --pretrain_dirs \\ ${ pretrain_dir } /Run_2.Dec20 \\ ${ pretrain_dir } /Run_2.Dec21 \\ ${ pretrain_dir } /Run_2.Dec22 \\ ${ pretrain_dir } /Run_2.Pretrain.20250505 \\ ${ pretrain_dir } /Run_2.Pretrain.20250507 \\ ${ pretrain_dir } /Run_3.Pretrain.20250527.Run2Extra \\ --store_dir ${ output_dir } /run.20250527.654M | tee ${ output_dir } /run.20250527.654M.log shifter python3 preprocessing/preprocess.py share/preprocess_pretrain.yaml \\ --c 60 \\ --pretrain_dirs \\ ${ pretrain_dir } /Combined_Balanced \\ --store_dir ${ output_dir } /run.20250625.2700M This command will: - Process all sub-runs in the three Run folders - Generate flattened .parquet files and normalization .pt file - Save logs to run.20250403.log Global Shuffle python3 NERSC/global_shuffle.py --first_shuffle_percent 1 .0 --second_shuffle_percent 0 .5 25 \\ /pscratch/sd/a/avencast/Event_Level_Analysis/Pretrain_Parquet/run.20250527.654M/","title":"\ud83d\ude80 Run on NERSC"},{"location":"preprocess_internal_only/#output-files","text":"Under the output directory ( --store_dir ), the script saves: /run.20250403/ \u251c\u2500\u2500 data_<timestamp>.parquet # Flattened input table \u251c\u2500\u2500 shape_metadata.json # Shape metadata for reconstruction \u251c\u2500\u2500 regression_norm.pt # Normalization statistics \u2514\u2500\u2500 run.20250403.log # Optional log if tee is used","title":"\ud83d\udce4 Output Files"},{"location":"preprocess_internal_only/#notes","text":"You must supply a valid preprocess_config.yaml (e.g., EveNet/share/preprocess_pretrain.yaml ). The script will skip any sub-run folder where no valid .h5 files are found or matching fails. All normalization is computed globally across all sub-runs and merged at the end.","title":"\u26a0\ufe0f Notes"},{"location":"train/","text":"Training \u00b6 \ud83d\ude80 Quickstart: Install the package with pip install evenet and run evenet-train <config.yaml> . Developing from source? Use python -m evenet.train <config.yaml> to pick up local edits. Loading and Saving Models \u00b6 This guide outlines how model weights and Exponential Moving Average (EMA) weights are handled in the training workflow. It supports both standard training continuation and pretraining-based initialization. \ud83d\udce6 YAML Configuration \u00b6 Specify the following fields under options.Training in your YAML config: model_checkpoint_save_path : \".\" # Directory to save Lightning checkpoints model_checkpoint_load_path : null # Path to resume training from a checkpoint (.ckpt) pretrain_model_load_path : null # Path to load pretrained model weights EMA : enable : true # Enable Exponential Moving Average tracking decay : 0.999 # Decay rate for EMA updates replace_model_after_load : false # Use EMA weights to overwrite model after load replace_model_at_end : true # Use EMA weights to overwrite model before saving \ud83d\udd01 Resuming Training from Checkpoint \u00b6 When model_checkpoint_load_path is provided, PyTorch Lightning automatically: Restores the model weights from checkpoint[\"state_dict\"] Resumes the optimizer, scheduler, and training state (e.g., global_step , current_epoch ) If EMA.enable: true , the training script additionally: Loads EMA weights from checkpoint[\"ema_state_dict\"] Optionally replaces the main model weights with EMA if EMA.replace_model_after_load: true \ud83d\ude80 Initializing from Pretrained Model \u00b6 When pretrain_model_load_path is specified, the system loads model weights during configure_model() using shape-validated safe loading: Only layers with matching names and shapes are loaded Incompatible layers are skipped with informative warnings This is suitable for transfer learning or domain adaptation tasks. Note on EMA: EMA weights are not loaded from the pretrained model If EMA.enable: true , the EMA model is initialized from the current model after loading \ud83d\udcc2 Saving Checkpoints \u00b6 When saving a checkpoint (e.g., at the end of training), Lightning includes: Model state dict Optimizer and scheduler state Training progress (epoch, global step, etc.) If EMA.replace_model_at_end: true , the system first copies EMA weights into the model before saving. This ensures the checkpoint reflects the EMA-smoothed model. \u2705 Summary of Loading and Saving Behavior \u00b6 Scenario YAML Setting Main Model Loaded EMA Loaded EMA Replaces Model Resume from checkpoint model_checkpoint_load_path \u2705 (automatic) \u2705 if EMA.enable \u2705 if EMA.replace_model_after_load Load from pretrained model pretrain_model_load_path \u2705 (safe-load) \u274c \u2705 if EMA.replace_model_after_load Save at end of training model_checkpoint_save_path \u2705 \u2705 if EMA.enable \u2705 if EMA.replace_model_at_end","title":"Training"},{"location":"train/#training","text":"\ud83d\ude80 Quickstart: Install the package with pip install evenet and run evenet-train <config.yaml> . Developing from source? Use python -m evenet.train <config.yaml> to pick up local edits.","title":"Training"},{"location":"train/#loading-and-saving-models","text":"This guide outlines how model weights and Exponential Moving Average (EMA) weights are handled in the training workflow. It supports both standard training continuation and pretraining-based initialization.","title":"Loading and Saving Models"},{"location":"train/#yaml-configuration","text":"Specify the following fields under options.Training in your YAML config: model_checkpoint_save_path : \".\" # Directory to save Lightning checkpoints model_checkpoint_load_path : null # Path to resume training from a checkpoint (.ckpt) pretrain_model_load_path : null # Path to load pretrained model weights EMA : enable : true # Enable Exponential Moving Average tracking decay : 0.999 # Decay rate for EMA updates replace_model_after_load : false # Use EMA weights to overwrite model after load replace_model_at_end : true # Use EMA weights to overwrite model before saving","title":"\ud83d\udce6 YAML Configuration"},{"location":"train/#resuming-training-from-checkpoint","text":"When model_checkpoint_load_path is provided, PyTorch Lightning automatically: Restores the model weights from checkpoint[\"state_dict\"] Resumes the optimizer, scheduler, and training state (e.g., global_step , current_epoch ) If EMA.enable: true , the training script additionally: Loads EMA weights from checkpoint[\"ema_state_dict\"] Optionally replaces the main model weights with EMA if EMA.replace_model_after_load: true","title":"\ud83d\udd01 Resuming Training from Checkpoint"},{"location":"train/#initializing-from-pretrained-model","text":"When pretrain_model_load_path is specified, the system loads model weights during configure_model() using shape-validated safe loading: Only layers with matching names and shapes are loaded Incompatible layers are skipped with informative warnings This is suitable for transfer learning or domain adaptation tasks. Note on EMA: EMA weights are not loaded from the pretrained model If EMA.enable: true , the EMA model is initialized from the current model after loading","title":"\ud83d\ude80 Initializing from Pretrained Model"},{"location":"train/#saving-checkpoints","text":"When saving a checkpoint (e.g., at the end of training), Lightning includes: Model state dict Optimizer and scheduler state Training progress (epoch, global step, etc.) If EMA.replace_model_at_end: true , the system first copies EMA weights into the model before saving. This ensures the checkpoint reflects the EMA-smoothed model.","title":"\ud83d\udcc2 Saving Checkpoints"},{"location":"train/#summary-of-loading-and-saving-behavior","text":"Scenario YAML Setting Main Model Loaded EMA Loaded EMA Replaces Model Resume from checkpoint model_checkpoint_load_path \u2705 (automatic) \u2705 if EMA.enable \u2705 if EMA.replace_model_after_load Load from pretrained model pretrain_model_load_path \u2705 (safe-load) \u274c \u2705 if EMA.replace_model_after_load Save at end of training model_checkpoint_save_path \u2705 \u2705 if EMA.enable \u2705 if EMA.replace_model_at_end","title":"\u2705 Summary of Loading and Saving Behavior"}]}